# Q3 - AB Testing

## Experiment Setup
### Hypothesis
Increasing the leads per job from 3 to 4 will increase lead utilization.

### Metrics
- Primary Metric: 
    - __Lead utilization: % of Leads Claimed/Leads Sent__
        - We want to understand whether Tradies are utilizing the leads sent to them at a higher rate for Group B.
- Secondary Metrics: 
    - __Lead Completion Rate: % of Jobs Completed/Leads Claimed__
        - When a Tradie claims a lead how likely do they win the job, complete it and get paid for it. If there are more leads being claimed then perhaps there will be more competition for each lead
    - __Job Completion Rate: % of Jobs Completed/Jobs Posted__ 
        - When user posts a job, how likely is that job gonna get completed. If there are more leads per job, perhaps there is a higher likelihood that the job will get complete as the user has more choices for tradies
    - __Avg. Lead Claims per Job__
        - The higher this number, the more revenue for hipages.

### Experiment Groups
- A: 3 leads per job
- B: 4 leads per job

### Enrolment criteria: 
Randomly bucket jobs in to A/B when the job is posted.

## Measurement Plan:
We would need to ensure the follow events are tracked either in the FE or BE:
- job_posted: Fired when the job is posted
- lead_sent: Fired when the lead is sent
- lead_claimed: Fired when the lead is claimed by the tradie
- job_completed: Fired when the Job is completed.

With the following properties:
- event_name: Name of the event
- job_id: ID of the job so we can join
- lead_id: ID of the lead
- tradie_id: ID of the tradie the lead got sent to
- category: The job category. e.g. plumbing/roofing/cleaning/etc.
- location: The Location of the job. e.g. Melbourne/Sydney
- experiment_group: Either A or B
- assigned_leads: Either 3 or 4
- timestamp: The timestamp of the event

Example of an event
```
{
  "event_name": "lead_sent",
  "lead_id": "lead_123",
  "tradie_id": "tradie_123",
  "job_id": "job_123",
  "category": "plumbing",
  "location": "Sydney",
  "assigned_leads": 3, 
  "experiment_group": "A",
  "timestamp": "2025-02-19T12:00:00Z"
}
```

## AB Test Execution:
1. Work with developers to ensure that the tracking is in place with the relevant properties. 
2. Test the events and make sure they work in nonprod environment.
3. Launch the experiment. 
4. Check whether data is flowing in to our data warehouse by building a dashboard.
5. Use a Bayesian AB testing approach to calculate likelihood of significant improvement. Typically I would recommend just rolling with an off the shelf experimentation tool like Amplitude OR Optimizely rather than writing your own experimentation tool from scratch.
6. Most likely need to run for atleast 2+ weeks, given the lead time for a job to get done.
7. Gather relevant stakeholders + PMs to analyze results and assess if there's any tradeoffs.
8. Make a decision on the AB Test. Go/No go

## Appendix
### Why Bayesian over frequentist stats?

Frequentist statistics rely only on the data collected during an experiment and makes no assumptions about what the impact could be, while Bayesian statistics start with an initial belief (prior knowledge) and update that belief as more data is collected. 

For an A/B test at Hipages, Bayesian is ideal because it allows us to continuously update our understanding of the impact of adding a 4th lead as we gather data, and make real-time decisions based on the latest evidence, rather than waiting for a fixed sample size to be reached.

This leads to faster times to reach a decision.